{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Fake News Classification"},{"metadata":{},"cell_type":"markdown","source":"## [1] Data Loading"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n#1: unreliable\n#0: reliable\ntrain=pd.read_csv('../input/train.csv')\ntest=pd.read_csv('../input/test.csv')\n\ntest['label']='t'\n","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## [2] Exploratory Data Analysis"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n#data prep\ntest=test.fillna(' ')\ntrain=train.fillna(' ')\ntest['total']=test['title']+' '+test['author']+test['text']\ntrain['total']=train['title']+' '+train['author']+train['text']\n\n\n\n","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"Index(['id', 'title', 'author', 'text', 'label', 'total'], dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.label.value_counts(normalize=True)","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"1    0.500625\n0    0.499375\nName: label, dtype: float64"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train.columns","execution_count":5,"outputs":[{"output_type":"execute_result","execution_count":5,"data":{"text/plain":"Index(['id', 'title', 'author', 'text', 'label', 'total'], dtype='object')"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"sorted_data=train.sort_values('id', axis=0, ascending=True, inplace=False, kind='quicksort', na_position='last')\n#Deduplication of entries\nfinal=sorted_data.drop_duplicates(subset={'title', 'author', 'text'}, keep='first', inplace=False)\nfinal.shape","execution_count":6,"outputs":[{"output_type":"execute_result","execution_count":6,"data":{"text/plain":"(20691, 6)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Checking data loss due to duplication\n(final['id'].size*1.0)/(train['id'].size*1.0)*100","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"99.47596153846155"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"# [3] Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"## Text Cleaning"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from bs4 import BeautifulSoup\nfrom tqdm import tqdm\nimport re\nfrom nltk.stem import SnowballStemmer\n","execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"snow=SnowballStemmer('english')","execution_count":9,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"## function to remove URl\ndef removeUrl(text):\n  pr=re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n  return pr\n## we also use Beautifull Soup to remove Html\ndef decontracted(phrase):\n    # specific\n    phrase = re.sub(r\"won't\", \"will not\", phrase)\n    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n\n    # general\n    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n    return phrase\n","execution_count":10,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"stopwords= set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n            \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n            'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n            'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n            'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n            'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n            'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n            'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n            'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n            's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n            've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n            \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n            \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n            'won', \"won't\", 'wouldn', \"wouldn't\"])","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final.head(2)","execution_count":12,"outputs":[{"output_type":"execute_result","execution_count":12,"data":{"text/plain":"   id                                              title           author  \\\n0   0  House Dem Aide: We Didn’t Even See Comey’s Let...    Darrell Lucus   \n1   1  FLYNN: Hillary Clinton, Big Woman on Campus - ...  Daniel J. Flynn   \n\n                                                text  label  \\\n0  House Dem Aide: We Didn’t Even See Comey’s Let...      1   \n1  Ever get the feeling your life circles the rou...      0   \n\n                                               total  \n0  House Dem Aide: We Didn’t Even See Comey’s Let...  \n1  FLYNN: Hillary Clinton, Big Woman on Campus - ...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>title</th>\n      <th>author</th>\n      <th>text</th>\n      <th>label</th>\n      <th>total</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n      <td>Darrell Lucus</td>\n      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n      <td>1</td>\n      <td>House Dem Aide: We Didn’t Even See Comey’s Let...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n      <td>Daniel J. Flynn</td>\n      <td>Ever get the feeling your life circles the rou...</td>\n      <td>0</td>\n      <td>FLYNN: Hillary Clinton, Big Woman on Campus - ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"## Fucntion to Remove Stop Words And  Perform Stemming"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"def removeStopWord(word):\n  token=word.split(\" \")   ## coverting string to token (list of word) \\\\ like [\"this\",\"is\",\"token\"]\n  removestop=[snow.stem(x) for x in token if x not in stopwords]   ##removing stopwords and also doing Stemming\n  removed=\" \".join(removestop)  ##joing back the list into sentence\n  return removed","execution_count":13,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning Train Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\npreprocessed_reviews = []\nfor line in tqdm(final.total.values):\n  line= BeautifulSoup(line, 'lxml').get_text() ## Remove Html Tags\n  line=removeUrl(line) #removing url\n  line=decontracted(line)    #Coverting word like { are't -> are not}\n  line = re.sub(r'[0-9]+', '', line)   ## To Remove Numbers from the string\n  line=line.lower()   ## Converting every word to lower case\n  line = re.sub(r'[^a-z0-9\\s]', '', line)   ## To clean all special Charaters\n  line=removeStopWord(line)    ## Removing Stop Words And doing Steaming\n  preprocessed_reviews.append(line.strip()) ## ading cleaned word into a list after removing spaces {By using strip()}","execution_count":14,"outputs":[{"output_type":"stream","text":"100%|██████████| 20691/20691 [02:46<00:00, 123.94it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"### Cleaning Test Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"from tqdm import tqdm\npreprocessed_test = []\nfor line in tqdm(test.total.values):\n  line= BeautifulSoup(line, 'lxml').get_text() ## Remove Html Tags\n  line=removeUrl(line) #removing url\n  line=decontracted(line)    #Coverting word like { are't -> are not}\n  line = re.sub(r'[0-9]+', '', line)   ## To Remove Numbers from the string\n  line=line.lower()   ## Converting every word to lower case\n  line = re.sub(r'[^a-z0-9\\s]', '', line)   ## To clean all special Charaters\n  line=removeStopWord(line)    ## Removing Stop Words And doing Steaming\n  preprocessed_test.append(line.strip()) ## ading cleaned word into a list after removing spaces {By using strip()}","execution_count":15,"outputs":[{"output_type":"stream","text":"100%|██████████| 5200/5200 [00:42<00:00, 121.83it/s]\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"##  Converting text into vectors using Tf-IDF"},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#tfidf\ntransformer = TfidfTransformer(smooth_idf=False)\ncount_vectorizer = CountVectorizer(ngram_range=(1, 2))\ncounts = count_vectorizer.fit_transform(preprocessed_reviews)\ntfidf = transformer.fit_transform(counts)\n","execution_count":16,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ### Preaparing Our Traning And Testing Data"},{"metadata":{"_uuid":"707f4b28946453b37bca62eec3321b8e7d28b436","_cell_guid":"3d389a36-c287-42bf-9db7-bf3a9669df50","trusted":true},"cell_type":"code","source":"targets = final['label'].values\ntest_counts = count_vectorizer.transform(preprocessed_test)\ntest_tfidf = transformer.fit_transform(test_counts)\n\n#split in samples\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(tfidf, targets, random_state=0)\n\n","execution_count":22,"outputs":[{"output_type":"stream","text":"/opt/conda/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:1067: RuntimeWarning: divide by zero encountered in true_divide\n  idf = np.log(float(n_samples) / df) + 1.0\n","name":"stderr"}]},{"metadata":{},"cell_type":"markdown","source":"# Creating Our Model"},{"metadata":{"_uuid":"feadf97a5a71b5769c373a71605bf6e355cd5dfc","_cell_guid":"38729daf-d919-4604-8c83-179f016a3332"},"cell_type":"markdown","source":"### Models We Are Going To Test\n* Classifications\n* Logistic regression\n* Decisiontree\n* KNeighbours\n* Linear Discriminant"},{"metadata":{"trusted":true,"_uuid":"d2029922ccefed1bce75ef44f9000913b284078f"},"cell_type":"code","source":"from sklearn.ensemble import (RandomForestClassifier, ExtraTreesClassifier,\n                              AdaBoostClassifier)\n\nExtr = ExtraTreesClassifier(n_estimators=5,n_jobs=4)\nExtr.fit(X_train, y_train)\nprint('Accuracy of ExtrTrees classifier on training set: {:.2f}'\n     .format(Extr.score(X_train, y_train)))\nprint('Accuracy of Extratrees classifier on test set: {:.2f}'\n     .format(Extr.score(X_test, y_test)))","execution_count":26,"outputs":[{"output_type":"stream","text":"Accuracy of ExtrTrees classifier on training set: 1.00\nAccuracy of Extratrees classifier on test set: 0.87\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"4a9a6465a986641aa1f927f28261ac8555f808bf"},"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\n\nAdab= AdaBoostClassifier(DecisionTreeClassifier(max_depth=3),n_estimators=5)\nAdab.fit(X_train, y_train)\nprint('Accuracy of Adaboost classifier on training set: {:.2f}'\n     .format(Adab.score(X_train, y_train)))\nprint('Accuracy of Adaboost classifier on test set: {:.2f}'\n     .format(Adab.score(X_test, y_test)))","execution_count":27,"outputs":[{"output_type":"stream","text":"Accuracy of Adaboost classifier on training set: 0.97\nAccuracy of Adaboost classifier on test set: 0.97\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"8bd92d2d80cc32556bab874c1accfd0bf0c96821"},"cell_type":"code","source":"Rando= RandomForestClassifier(n_estimators=5)\n\nRando.fit(X_train, y_train)\nprint('Accuracy of randomforest classifier on training set: {:.2f}'\n     .format(Rando.score(X_train, y_train)))\nprint('Accuracy of randomforest classifier on test set: {:.2f}'\n     .format(Rando.score(X_test, y_test)))","execution_count":28,"outputs":[{"output_type":"stream","text":"Accuracy of randomforest classifier on training set: 0.99\nAccuracy of randomforest classifier on test set: 0.84\n","name":"stdout"}]},{"metadata":{"_uuid":"55ab5aeb591e44adcb9144c2c9c6082270608112","_cell_guid":"311b6c59-5477-407e-9b6f-9d644c872267","trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB\n\nNB = MultinomialNB()\nNB.fit(X_train, y_train)\nprint('Accuracy of NB  classifier on training set: {:.2f}'\n     .format(NB.score(X_train, y_train)))\nprint('Accuracy of NB classifier on test set: {:.2f}'\n     .format(NB.score(X_test, y_test)))\n","execution_count":24,"outputs":[{"output_type":"stream","text":"Accuracy of NB  classifier on training set: 0.95\nAccuracy of NB classifier on test set: 0.84\n","name":"stdout"}]},{"metadata":{"trusted":true,"_uuid":"991ce56b986d25e6ef6c61535fd94c41be828045"},"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression(C=1e5)\n\nlogreg.fit(X_train, y_train)\nprint('Accuracy of Lasso classifier on training set: {:.2f}'\n     .format(logreg.score(X_train, y_train)))\nprint('Accuracy of Lasso classifier on test set: {:.2f}'\n     .format(logreg.score(X_test, y_test)))","execution_count":23,"outputs":[{"output_type":"stream","text":"Accuracy of Lasso classifier on training set: 1.00\nAccuracy of Lasso classifier on test set: 0.99\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"#### From the above we can see that our Logistic Regression is the Best Model that we can use.\n#### So Lets Train All Our Data Set Using This Model"},{"metadata":{"_uuid":"14a0bd63310ff47390c66618ad4cc6c92910a7a5","_cell_guid":"365b59da-b511-47ce-9965-4dc16197e237","trusted":true},"cell_type":"code","source":"targets = final['label'].values\nlogreg = LogisticRegression()\nlogreg.fit(counts, targets)\n\n","execution_count":29,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":"LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n          verbose=0, warm_start=False)"},"metadata":{}}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"# Vectorizing Our Test Data\nexample_counts = count_vectorizer.transform(preprocessed_test)","execution_count":30,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Prediction It On Test data And Saving The Result In A DataFrame"},{"metadata":{"trusted":true},"cell_type":"code","source":"\npredictions = logreg.predict(example_counts)\n\npred=pd.DataFrame(predictions,columns=['label'])\npred['id']=test['id']\npred.groupby('label').count()","execution_count":31,"outputs":[{"output_type":"execute_result","execution_count":31,"data":{"text/plain":"         id\nlabel      \n0      2599\n1      2601","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2599</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2601</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"> Converting DataFrame To Csv For Submission"},{"metadata":{"_uuid":"63673cfcef9abdfe7cdc859ab1c9a279bddbde09","_cell_guid":"d8589f7b-ba90-4bb6-bd80-fce957373eb1","collapsed":true,"trusted":true},"cell_type":"code","source":"pred.to_csv('submition.csv', index=False)\n","execution_count":32,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# # # Finally Lets Look How Our Submission Looks Like"},{"metadata":{"trusted":true},"cell_type":"code","source":"pred.head(5)","execution_count":33,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":"   label     id\n0      0  20800\n1      1  20801\n2      1  20802\n3      0  20803\n4      1  20804","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>20800</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>20801</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>20802</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>20803</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>20804</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}